{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depends on:\n",
    "\n",
    "A0. Distress Topic Modeling notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_FILE = \"../working-dir/parsed/classified_comments/sample_total_comments_2018.tsv\"\n",
    "FLATTENED_TREES_FILE = \"../working-dir/parsed/classified_comments/flattened_comment_trees_all_2018.tsv\"\n",
    "LIWC_DICT_FILE = \"/home/REDACTED/LIWC2015_English.dic\"\n",
    "POST_METADATA_FILE = \"../working-dir/parsed/classified_comments/post_metadata_total_2018.tsv\"\n",
    "GRIEF_TOPICS_PATH = \"../working-dir/mallet/seeking_topics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/IPython/config.py:13: ShimWarning: The `IPython.config` package has been deprecated since IPython 4.0. You should import from traitlets.config instead.\n",
      "  \"You should import from traitlets.config instead.\", ShimWarning)\n",
      "/home/naitian/.local/lib/python3.7/site-packages/ipycache.py:17: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils.traitlets import Unicode\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%load_ext ipycache\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0xB1AB)\n",
    "# from bert_classifier.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import read_labeled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled comments from ../working-dir/parsed/classified_comments/sample_total_comments_2018.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "(year,\n",
    " c,\n",
    " cs,\n",
    " both) = read_labeled_output(COMMENTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trees(fname):\n",
    "    tree = dict()\n",
    "    with open(fname, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            parent = parts[0]\n",
    "            children = parts[1:]\n",
    "            tree[parent] = children\n",
    "    return tree\n",
    "tree = read_trees(FLATTENED_TREES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "16000000\n",
      "17000000\n",
      "18000000\n",
      "19000000\n",
      "20000000\n",
      "21000000\n",
      "22000000\n",
      "23000000\n",
      "24000000\n",
      "25000000\n",
      "26000000\n",
      "27000000\n",
      "28000000\n",
      "29000000\n",
      "30000000\n",
      "31000000\n",
      "32000000\n",
      "33000000\n",
      "34000000\n",
      "35000000\n"
     ]
    }
   ],
   "source": [
    "grief_ids = set(cs.index)\n",
    "condolence_ids = set(c.index)\n",
    "both = grief_ids.intersection(condolence_ids)\n",
    "\n",
    "grief_ids -= both\n",
    "condolence_ids -= both\n",
    "\n",
    "grief_dict = cs.to_dict()\n",
    "condolence_dict = c.to_dict()\n",
    "\n",
    "no_response = []\n",
    "no_condolence = []\n",
    "condolence = []\n",
    "\n",
    "pairs = []\n",
    "for i, g_id in enumerate(tree):\n",
    "    if i % 1_000_000 == 0:\n",
    "        print(i)\n",
    "    if g_id in grief_ids:\n",
    "        c_replies = [c_id for c_id in tree[g_id] if c_id in condolence_ids]\n",
    "        if len(tree[g_id]) == 0:\n",
    "            no_response.append((g_id, []))\n",
    "        elif len(c_replies) > 0:\n",
    "            condolence.append((g_id, c_replies))\n",
    "        else:\n",
    "            no_condolence.append((g_id, tree[g_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grief_topics = pd.read_csv(GRIEF_TOPICS_PATH, sep=\"\\t\", names=[\"index\",\n",
    "                                                               \"id\",\n",
    "                                                               \"has_diet\",\n",
    "                                                               \"has_cursing\",\n",
    "                                                               \"who_knows_1\",\n",
    "                                                               \"has_social_romantic\",\n",
    "                                                               \"has_business\",\n",
    "                                                               \"has_games\",\n",
    "                                                               \"has_technology\",\n",
    "                                                               \"has_family\",\n",
    "                                                               \"who_knows_2\",\n",
    "                                                               \"has_sports\",\n",
    "                                                               \"has_temporal\",\n",
    "                                                               \"has_politics\",\n",
    "                                                               \"has_career\",\n",
    "                                                               \"has_finance\",\n",
    "                                                               \"has_nostalgia\",\n",
    "                                                               \"has_pet\",\n",
    "                                                               \"has_health\",\n",
    "                                                               \"has_anecdote\",\n",
    "                                                               \"has_mental_health\",\n",
    "                                                               \"has_home_moving\"], index_col=\"id\")\n",
    "grief_topics = grief_topics.drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genderperformr\n",
    "\n",
    "def get_gender_scores(usernames):\n",
    "    gender_scores = np.array([])\n",
    "    k = 10_000\n",
    "    for i in range(0, len(usernames), k):\n",
    "        print(i)\n",
    "        gender_scores = np.concatenate([gender_scores, genderperformr.predict(usernames.iloc[i:i + k].to_list())[0]])\n",
    "    gender_scores = gender_scores[:len(usernames)]\n",
    "    return gender_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get comment depth...\n",
    "\n",
    "id_to_depth = dict()\n",
    "\n",
    "def set_depth(grief_id, depth=0):\n",
    "    if grief_id in id_to_depth:\n",
    "        return\n",
    "    id_to_depth[grief_id] = depth\n",
    "    for child in tree[grief_id]:\n",
    "        set_depth(child, depth + 1)\n",
    "\n",
    "for i, grief_id in enumerate(tree):\n",
    "    if i % 1_000_000 == 0:\n",
    "        print(i)\n",
    "    set_depth(grief_id, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "import re\n",
    "parse, category_names = liwc.load_token_parser(LIWC_DICT_FILE)\n",
    "def tokenize(text):\n",
    "    # you may want to use a smarter tokenizer\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_ling_accommodation(parent_id, child_id):\n",
    "    parent_comment = grief_dict[\"body\"][parent_id].lower()\n",
    "    child_comment = condolence_dict[\"body\"][child_id].lower()\n",
    "    parent_words = set(word_tokenize(parent_comment))\n",
    "    child_words = set(word_tokenize(child_comment))\n",
    "    return len(parent_words.intersection(child_words)) / len(child_words)\n",
    "        \n",
    "def get_liwc(body):\n",
    "    liwc_first_person = 0\n",
    "    liwc_second_person = 0\n",
    "    liwc_third_person = 0\n",
    "    for tok in tokenize(body.lower()):\n",
    "        for category in parse(tok):\n",
    "            if category == \"i (I)\" or category == \"we (We)\":\n",
    "                liwc_first_person += 1\n",
    "            elif category == \"you (You)\":\n",
    "                liwc_second_person += 1\n",
    "            elif category == \"shehe (SheHe)\" or category == \"they (They)\":\n",
    "                liwc_third_person += 1\n",
    "    return liwc_first_person, liwc_second_person, liwc_third_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data = pd.read_csv(POST_METADATA_FILE,\n",
    "                        names=[\"post_id\",\n",
    "                               \"author\",\n",
    "                               \"is_self\",\n",
    "                               \"created_utc\",\n",
    "                               \"score\"],\n",
    "                        index_col=\"post_id\",\n",
    "                        sep=\"\\t\")\n",
    "post_data[\"created_utc\"] = pd.to_datetime(post_data.created_utc, unit=\"s\", utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_in_tree = grief_ids.intersection(set(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False\n",
    "dataset = cs[[\"subreddit\", \"link_id\", \"body\", \"seeking_score\", \"score\", \"created_utc\", \"author\"]].loc[sample(ids_in_tree, 1_000_000)]\n",
    "\n",
    "print(\"Calculating Length\")\n",
    "# Length in space-delimited words\n",
    "dataset.loc[:, (\"length\")] = dataset.body.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "print(\"Filter Link IDs\")\n",
    "counts = dataset.link_id.value_counts(ascending=False)\n",
    "links = set((counts[counts > 50]).index)\n",
    "dataset.loc[:, \"filtered_link_id\"] = dataset.loc[:, \"link_id\"].apply(lambda x: x if x in links else \"dummy\")\n",
    "\n",
    "print(\"Gender Scores\")\n",
    "gender_scores = get_gender_scores(dataset.author)\n",
    "dataset.loc[:, (\"is_male\")] = gender_scores < 0.1\n",
    "dataset.loc[:, (\"is_female\")] = gender_scores > 0.9\n",
    "\n",
    "print(\"Post Features\")\n",
    "dataset.loc[:, \"link_id_stripped\"] = dataset.loc[:, \"link_id\"].apply(lambda x: x[3:])\n",
    "dataset = dataset.join(post_data, on=\"link_id_stripped\", rsuffix=\"_post\")\n",
    "\n",
    "print(\"Temporal\")\n",
    "# set dataset timezone\n",
    "dataset.loc[:, \"created_utc\"] = dataset.created_utc.dt.tz_convert(\"US/Eastern\")\n",
    "dataset.loc[:, \"created_utc_post\"] = dataset.created_utc_post.dt.tz_convert(\"US/Eastern\")\n",
    "dataset.loc[:, \"hour\"] = dataset.created_utc.dt.hour\n",
    "dataset.loc[:, \"month\"] = dataset.created_utc.dt.month\n",
    "dataset.loc[:, \"day_of_month\"] = dataset.created_utc.dt.day\n",
    "dataset.loc[:, \"weekday\"] = dataset.created_utc.dt.weekday\n",
    "\n",
    "print(\"Topics\")\n",
    "# Before running this, make sure to run topic modeling code below.\n",
    "dataset = dataset.join(grief_topics)\n",
    "\n",
    "print(\"Comment Depth\")\n",
    "dataset.loc[:, \"depth\"] = np.array([id_to_depth[grief_id] for grief_id in dataset.index])\n",
    "\n",
    "print(\"Change Score Type\")\n",
    "dataset.loc[:, \"score\"] = dataset.loc[:, \"score\"].astype(np.float)\n",
    "\n",
    "print(\"Comment Age in hours\")\n",
    "dataset.loc[:, \"comment_age\"] = ((dataset.loc[:, \"created_utc\"] - dataset.loc[:, \"created_utc_post\"]) / np.timedelta64(1, 'h'))\n",
    "\n",
    "\n",
    "print(\"LIWC Features\")\n",
    "dataset.loc[:, \"liwc_first_person\"], dataset.loc[:, \"liwc_second_person\"], dataset.loc[:, \"liwc_third_person\"] = zip(*dataset.body.apply(get_liwc))\n",
    "\n",
    "\n",
    "print(\"Response Value\")\n",
    "# Add response values\n",
    "no_response_ids = set([g_id for g_id, _ in no_response])\n",
    "condolence_ids = set([g_id for g_id, _ in condolence])\n",
    "dataset.loc[:, \"received_reply\"] = [ind not in no_response_ids for ind in dataset.index.values]\n",
    "dataset.loc[:, \"received_condolence\"] = [ind in condolence_ids for ind in dataset.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"../data/regression/distress_input.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
