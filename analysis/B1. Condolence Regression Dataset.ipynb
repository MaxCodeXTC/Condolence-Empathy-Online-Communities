{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies:\n",
    "\n",
    "B0. Condolence Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENTS_FILE = \"../working-dir/parsed/classified_comments/sample_total_comments_2018.tsv\"\n",
    "FLATTENED_TREES_FILE = \"../working-dir/parsed/classified_comments/flattened_comment_trees_all_2018.tsv\"\n",
    "LIWC_DICT_FILE = \"/home/REDACTED/LIWC2015_English.dic\"\n",
    "POST_METADATA_FILE = \"../working-dir/parsed/classified_comments/post_metadata_total_2018.tsv\"\n",
    "GRIEF_TOPICS_PATH = \"../working-dir/mallet/seeking_topics.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import ujson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%load_ext ipycache\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0xB1AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import read_labeled_output\n",
    "(year,\n",
    " c,\n",
    " cs,\n",
    " both) = read_labeled_output(COMMENTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_topics = pd.read_csv(\"./mallet/condolences_topics.txt\", sep=\"\\t\", names=[\"index\",\n",
    "                                                                           \"id\",\n",
    "                                                                           \"0\",\n",
    "                                                                           \"1\",\n",
    "                                                                           \"2\",\n",
    "                                                                           \"3\",\n",
    "                                                                           \"4\",\n",
    "                                                                           \"5\",\n",
    "                                                                           \"6\",\n",
    "                                                                           \"7\",\n",
    "                                                                           \"8\",\n",
    "                                                                           \"9\",\n",
    "                                                                           \"10\",\n",
    "                                                                           \"11\",\n",
    "                                                                           \"12\",\n",
    "                                                                           \"13\",\n",
    "                                                                           \"14\",\n",
    "                                                                           \"15\",\n",
    "                                                                           \"16\",\n",
    "                                                                           \"17\",\n",
    "                                                                           \"18\",\n",
    "                                                                           \"19\"], index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trees(fname):\n",
    "    tree = dict()\n",
    "    with open(fname, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            parent = parts[0]\n",
    "            children = parts[1:]\n",
    "            tree[parent] = children\n",
    "    return tree\n",
    "tree = read_trees(FLATTENED_TREES_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = pd.DataFrame([ujson.loads(line.strip())\n",
    "                        for line in open(\"./parsed/classified_comments/replies_to_condolence_total.tsv\", \"r\")])\n",
    "replies = replies.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\"thanks\", \"thank you\", \"i appreciate\", \"crying just reading this\", \"made my day\", ]\n",
    "\n",
    "replies.loc[:, \"positive_response\"] = replies.body.str.lower().apply(lambda x: any([p in x for p in phrases]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grief_ids = set(cs.index)\n",
    "condolence_ids = set(c.index)\n",
    "both = grief_ids.intersection(condolence_ids)\n",
    "\n",
    "reply_ids = set(replies.index)\n",
    "reply_dict = replies.to_dict()\n",
    "\n",
    "condolence_to_grief_map = dict()\n",
    "reply_to_condolence_map = dict()\n",
    "\n",
    "for i, c_id in enumerate(condolence_ids):\n",
    "    if i % 100_000 == 0:\n",
    "        print(i)\n",
    "    if condolence_dict[\"parent_id\"][c_id][:3] == \"t3_\":\n",
    "        continue\n",
    "    parent_comment = condolence_dict[\"parent_id\"][c_id][3:]\n",
    "    if parent_comment in grief_ids:\n",
    "        condolence_to_grief_map[c_id] = parent_comment\n",
    "\n",
    "for i, r_id in enumerate(reply_ids):\n",
    "    if i % 100_000 == 0:\n",
    "        print(i)\n",
    "    if reply_dict[\"parent_id\"][r_id][:3] == \"t3_\":\n",
    "        continue\n",
    "    parent_comment = reply_dict[\"parent_id\"][r_id][3:]\n",
    "    if parent_comment in condolence_ids:\n",
    "        reply_to_condolence_map[r_id] = parent_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_reply_from_op(row):\n",
    "    if row.name not in reply_to_condolence_map:\n",
    "        return False\n",
    "    if reply_to_condolence_map[row.name] in condolence_to_grief_map:\n",
    "        return grief_dict[\"author\"][condolence_to_grief_map[reply_to_condolence_map[row.name]]] == row.author\n",
    "    return False\n",
    "\n",
    "replies.loc[:, \"from_op\"] = replies.apply(is_reply_from_op, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_condolences = set([p_id[3:] for p_id in replies[replies.from_op & replies.positive_response].parent_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there aren't a lot of good condolences, so we add the all in, and randomly sample other ones until we hit 1mil\n",
    "# random_id_sample = good_condolences.union(random.sample(set(condolence_to_grief_map.keys()) - good_condolences, k=(1_000_000 - len(good_condolences))))\n",
    "\n",
    "# since there aren't a lot of condolence comments that are in reply to a condolence seeking comment, we use all of them\n",
    "# so, the \"random_id_sample\" isn't actually random, but ok.\n",
    "random_id_sample = condolence_to_grief_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "import re\n",
    "parse, category_names = liwc.load_token_parser(LIWC_DICT_FILE)\n",
    "def tokenize(text):\n",
    "    # you may want to use a smarter tokenizer\n",
    "    for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "        yield match.group(0)\n",
    "\n",
    "        \n",
    "def get_liwc(body):\n",
    "    liwc_first_person = 0\n",
    "    liwc_second_person = 0\n",
    "    liwc_third_person = 0\n",
    "    for tok in tokenize(body.lower()):\n",
    "        for category in parse(tok):\n",
    "            if category == \"i (I)\" or category == \"we (We)\":\n",
    "                liwc_first_person += 1\n",
    "            elif category == \"you (You)\":\n",
    "                liwc_second_person += 1\n",
    "            elif category == \"shehe (SheHe)\" or category == \"they (They)\":\n",
    "                liwc_third_person += 1\n",
    "    return liwc_first_person, liwc_second_person, liwc_third_person\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_ling_accommodation(parent_id, child_id):\n",
    "    parent_comment = grief_dict[\"body\"][parent_id].lower()\n",
    "    child_comment = condolence_dict[\"body\"][child_id].lower()\n",
    "    parent_words = set(word_tokenize(parent_comment))\n",
    "    child_words = set(word_tokenize(child_comment))\n",
    "    return len(parent_words.intersection(child_words)) / len(child_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features:\n",
    "\n",
    "\n",
    "# LIWC features\n",
    "\n",
    "# all the normal controls\n",
    "\n",
    "# topics\n",
    "\n",
    "# speech acts\n",
    "\n",
    "\n",
    "dataset = c[[\"subreddit\", \"link_id\", \"body\", \"score\", \"created_utc\", \"author\"]].loc[random_id_sample]\n",
    "dataset = dataset.drop_duplicates()\n",
    "print(\"Calculating Length\")\n",
    "# Length in space-delimited words\n",
    "dataset.loc[:, (\"length\")] = dataset.body.apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "print(\"Filter Link IDs\")\n",
    "counts = dataset.link_id.value_counts(ascending=False)\n",
    "links = set((counts[counts > 50]).index)\n",
    "dataset.loc[:, \"filtered_link_id\"] = dataset.loc[:, \"link_id\"].apply(lambda x: x if x in links else \"dummy\")\n",
    "\n",
    "print(\"Gender Scores\")\n",
    "gender_scores = get_gender_scores(dataset.author)\n",
    "dataset.loc[:, (\"is_male\")] = gender_scores < 0.1\n",
    "dataset.loc[:, (\"is_female\")] = gender_scores > 0.9\n",
    "\n",
    "print(\"Post Features\")\n",
    "dataset.loc[:, \"link_id_stripped\"] = dataset.loc[:, \"link_id\"].apply(lambda x: x[3:])\n",
    "dataset = dataset.join(post_data, on=\"link_id_stripped\", rsuffix=\"_post\")\n",
    "\n",
    "print(\"Temporal\")\n",
    "# set dataset timezone\n",
    "dataset.loc[:, \"created_utc\"] = dataset.created_utc.dt.tz_convert(\"US/Eastern\")\n",
    "dataset.loc[:, \"created_utc_post\"] = dataset.created_utc_post.dt.tz_convert(\"US/Eastern\")\n",
    "dataset.loc[:, \"hour\"] = dataset.created_utc.dt.hour\n",
    "dataset.loc[:, \"month\"] = dataset.created_utc.dt.month\n",
    "dataset.loc[:, \"day_of_month\"] = dataset.created_utc.dt.day\n",
    "dataset.loc[:, \"weekday\"] = dataset.created_utc.dt.weekday\n",
    "\n",
    "print(\"Topics\")\n",
    "# Before running this, make sure to run topic modeling code below.\n",
    "dataset = dataset.join(c_topics)\n",
    "\n",
    "# print(\"Comment Depth\")\n",
    "# dataset.loc[:, \"depth\"] = np.array([id_to_depth[condolence_to_grief_map[c_id]] + 1 for c_id in dataset.index])\n",
    "\n",
    "# print(\"Comment Age in hours\")\n",
    "# dataset.loc[:, \"comment_age\"] = ((dataset.loc[:, \"created_utc\"] - dataset.loc[:, \"created_utc_post\"]) / np.timedelta64(1, 'h'))\n",
    "dataset.loc[:, \"comment_age\"] = ((dataset.loc[:, \"created_utc\"] - grief_dict[\"created_utc\"][condolence_to_grief_map[c_id]]) / np.timedelta64(1, 'h'))\n",
    "\n",
    "print(\"Linguistic Alignment\")\n",
    "dataset.loc[:, \"ling_align\"] = np.array([get_ling_accommodation(condolence_to_grief_map[c_id], c_id) for c_id in dataset.index])\n",
    "\n",
    "print(\"LIWC Features\")\n",
    "dataset.loc[:, \"liwc_first_person\"], dataset.loc[:, \"liwc_second_person\"], dataset.loc[:, \"liwc_third_person\"] = zip(*dataset.body.apply(get_liwc))\n",
    "\n",
    "\n",
    "print(\"Response Value\")\n",
    "# Add response values\n",
    "dataset.loc[:, \"good_condolence\"] = [ind in good_condolences for ind in dataset.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"../data/regression/condolence_input.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
